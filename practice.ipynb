{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devide time-series data into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data, split_rate = 0.7):\n",
    "    train_size = int(len(data) * split_rate)\n",
    "    train_data, test_data = data[:train_size], data[train_size:]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## Split the sequence\n",
    "## Function to create sequences of data\n",
    "def create_sequences(data, seq_length):\n",
    "    sequence = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        one_data = (torch.tensor(data[i:i+seq_length]), data[i+seq_length])\n",
    "        sequence.append(one_data)\n",
    "    # print(sequence)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader(data, seq_length = 3, split_rate_train_test = 0.7, split_rate_train_val = 0.7, batch_size = 1, shuffle_train = False, shuffle_val = False, shuffle_test = False):\n",
    "    '''\n",
    "    Input the data, return training dataloader, validation dataloader, and test dataloader\n",
    "    '''\n",
    "    train_data, test_data = data_split(data, split_rate_train_test)\n",
    "    train_data, val_data = data_split(train_data, split_rate_train_val)\n",
    "\n",
    "    training_data_seq = create_sequences(train_data, seq_length)\n",
    "    val_data_seq = create_sequences(val_data, seq_length)\n",
    "    test_data_seq = create_sequences(test_data, seq_length)\n",
    "\n",
    "    train_dataloader = DataLoader(training_data_seq, batch_size, shuffle = shuffle_train) ## Change to true later\n",
    "    val_dataloader = DataLoader(val_data_seq, batch_size, shuffle = shuffle_val)\n",
    "    test_dataloader = DataLoader(test_data_seq, batch_size, shuffle = shuffle_test)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_loader(train_loader):\n",
    "    '''\n",
    "    a function shows every entry of train_loader\n",
    "    '''\n",
    "    for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            print(\"inputs: \", inputs, \"labels: \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a simple linear model\n",
    "class linear_Net(nn.Module):\n",
    "    '''\n",
    "    A very simple linear model\n",
    "    '''\n",
    "    def __init__(self, seq_unit, output_size = 1):\n",
    "        super().__init__()\n",
    "        self.seq_unit = seq_unit\n",
    "        self.fc1 = nn.Linear(seq_unit, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_Net(nn.Module):\n",
    "    '''\n",
    "    A basic multiple linear perceptron model with a hidden layer\n",
    "    '''\n",
    "    def __init__(self, seq_unit, hidden_size, output_size = 1):\n",
    "        super().__init__()\n",
    "        self.activation = F.relu\n",
    "        self.fc1 = nn.Linear(seq_unit, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_Net(nn.Module):\n",
    "    '''\n",
    "    A basic convolutional neural network with a number of filters\n",
    "    '''\n",
    "    def __init__(self, seq_unit, kernel_size = 3, output_size = 1):\n",
    "        super().__init__()\n",
    "        self.seq_unit = seq_unit\n",
    "        self.conv_net = nn.Sequential(\n",
    "            ## vector size: 3\n",
    "            nn.Conv1d(in_channels=1, out_channels= 16, kernel_size=kernel_size, stride=1, padding=1), ## 4x16\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(seq_unit*16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## permute to put channel in correct order\n",
    "        # x = x.permute(0, 2, 1)\n",
    "        out = self.conv_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the LSTM\n",
    "class lstm_Net(nn.Module):\n",
    "    '''\n",
    "    A long short term memory model\n",
    "    '''\n",
    "    def __init__(self, seq_unit, hidden_size = 32, output_size = 1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(seq_unit, hidden_size, num_layers = 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        # x = self.fc(x[:, -1, :]) ## extract only the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(model, loss_function, x, y, optimizer = None):\n",
    "    '''\n",
    "    Apply loss function to a batch of inputs. If no optimizer is provided, \n",
    "    skip the back propagation step\n",
    "    '''\n",
    "    ## prediction\n",
    "    # print(\"         Enter function model_loss:\\n\")\n",
    "    output = model(x.float())\n",
    "    # print(\"             output:\", output)\n",
    "\n",
    "    loss = loss_function(output, y.float())\n",
    "\n",
    "    if optimizer is not None:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # print(\"         model_loss:\", loss.item(), \"\\n\")\n",
    "    return loss.item(), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dl, loss_function, device, optimizer):\n",
    "    '''\n",
    "    Execute 1 set of batched training within an epoch\n",
    "    '''\n",
    "    # print(\"     Enter function train_one_epoch():\\n\")\n",
    "    ## Set the model to training mode\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    batch_sizes = []\n",
    "\n",
    "    ## Loop through train dataloader\n",
    "    for x_train, y_train in train_dl:\n",
    "        # print(\"train data x: \", x_train)\n",
    "        # print(\"train data y: \", y_train)\n",
    "        ## transfer the data to GPU if any\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "\n",
    "        ## Back propagation\n",
    "        train_loss, batch_size = model_loss(model, loss_function, x_train, y_train, optimizer)\n",
    "\n",
    "        ## Append train loss and batch size\n",
    "        train_losses.append(train_loss)\n",
    "        batch_sizes.append(batch_size)\n",
    "    ## Calculate the average losses over all batches\n",
    "    train_loss = np.sum(np.multiply(train_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "    # print(\"batch train loss: \", train_loss)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(model,val_dl,loss_function,device, optimizer):\n",
    "    '''\n",
    "    Excute 1 set of batched validation within an epoch\n",
    "    '''\n",
    "    # print(\"     Enter function val_one_epoch():\\n\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validation_losses = []\n",
    "        batch_sizes = []\n",
    "\n",
    "        ## Loop through the validation dataloader\n",
    "        for x_valid, y_valid in val_dl:\n",
    "            ## transfer the data to GPU if any\n",
    "            x_valid, y_valid = x_valid.to(device), y_valid.to(device)\n",
    "\n",
    "            ## Calculate the loss WITHOUT BACK PROPOGATION\n",
    "            validation_loss, batch_size = model_loss(model, loss_function, x_valid, y_valid)\n",
    "\n",
    "            ## Append validation loss and batch size\n",
    "            validation_losses.append(validation_loss)\n",
    "            batch_sizes.append(batch_size)\n",
    "\n",
    "        ## Calculate the average losses over all batches\n",
    "        val_loss = np.sum(np.multiply(validation_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "        # print(\"batch validation loss: \", val_loss)\n",
    "\n",
    "        return val_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the model with testing data\n",
    "def test_model(model, test_dl, loss_function, optimizer):\n",
    "    # Evaluate the model on the test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_losses = []\n",
    "        batch_sizes = []\n",
    "\n",
    "        ## Loop through the validation dataloader\n",
    "        for x_test, y_test in test_dl:\n",
    "            # print(\"test data x: \", x_test)\n",
    "            # print(\"test data y: \", y_test)\n",
    "\n",
    "            ## Calculate the loss WITHOUT BACK PROPOGATION\n",
    "            test_loss, batch_size = model_loss(model, loss_function, x_test, y_test)\n",
    "\n",
    "            ## Append validation loss and batch size\n",
    "            test_losses.append(test_loss)\n",
    "            batch_sizes.append(batch_size)\n",
    "\n",
    "        ## Calculate the average losses over all batches\n",
    "        test_loss = np.sum(np.multiply(test_losses, batch_sizes)) / np.sum(batch_sizes)\n",
    "\n",
    "        return test_loss\n",
    "    print('Test Loss: {:.4f}'.format(test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(train_dl, val_dl, model, device, num_epochs, loss_function, optimizer):\n",
    "    '''\n",
    "    For num_epochs, fit the model parameters to the training data, evaluation on validation data\n",
    "    For each epoch, calculate and return the training and validation loss\n",
    "    '''\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        ## step 1: training\n",
    "        train_loss = train_one_epoch(model, train_dl, loss_function, device, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        ## step 2: validation\n",
    "        val_loss = val_one_epoch(model, val_dl, loss_function, device, optimizer)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(train_dl, val_dl, test_dl, model, device, learning_rate = 0.001, num_epochs = 500, loss_function = None, optimizer = None):\n",
    "    ## Define loss function\n",
    "    if loss_function is None:\n",
    "        loss_function = nn.MSELoss()\n",
    "    \n",
    "    ## Define optimization function\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    ## run the model fit function\n",
    "    train_losses, val_losses = model_fit(train_dl, val_dl, model, device, num_epochs, loss_function, optimizer)\n",
    "\n",
    "    ## test the model\n",
    "    test_losses = test_model(model, test_dl, loss_function, optimizer)\n",
    "\n",
    "    return train_losses, val_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read data/generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate artificial data\n",
    "def generate_data(num_data = 500, step = 1, func = \"Linear\", pattern = 1):\n",
    "    '''\n",
    "    Generate artificial data with different patterns \n",
    "\n",
    "    function 1: linear\n",
    "    function 2: sin(x)\n",
    "\n",
    "    pattern 0: return data as it is\n",
    "    pattern 1: add a mask every 5 numbers\n",
    "    pattern 2: add a linear function\n",
    "    '''\n",
    "    ## generate the index from 0 to num_data\n",
    "    idx = np.arange(0, num_data) \n",
    "\n",
    "    ## determine the function\n",
    "    if (func == \"Linear\"):\n",
    "        initial_data = idx\n",
    "    elif (func == \"Sin\"):\n",
    "        initial_data = np.sin(idx)\n",
    "    \n",
    "    ## determine the patterns\n",
    "    if(pattern == 0):\n",
    "        return initial_data\n",
    "        \n",
    "    for idx in range(0, len(initial_data)):\n",
    "        if(idx % 50 == 0):\n",
    "            if(pattern == 1):\n",
    "                if(idx - 4 >= 0): initial_data[idx] += 1\n",
    "                if(idx - 3 >= 0): initial_data[idx] += 2\n",
    "                if(idx - 2 >= 0): initial_data[idx] += 3\n",
    "                if(idx - 1 >= 0): initial_data[idx] += 4\n",
    "                if(idx >= 0 and idx < len(initial_data)): initial_data[idx] += 5\n",
    "                if(idx + 1 < len(initial_data)): initial_data[idx] += 4\n",
    "                if(idx + 2 < len(initial_data)): initial_data[idx] += 3\n",
    "                if(idx + 3 < len(initial_data)): initial_data[idx] += 2\n",
    "                if(idx + 4 < len(initial_data)): initial_data[idx] += 1\n",
    "                \n",
    "            elif(pattern == 2):\n",
    "                initial_data[idx] = initial_data[idx]*5 + 8\n",
    "\n",
    "    return initial_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(dataset = \"EVcharging_data.csv\"):\n",
    "    '''\n",
    "    Import existing data from directory\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(datatype = 1, num_data = 500, step = 1, func = \"Linear\", pattern = 1, dataset = \"EVCharging_data.csv\"):\n",
    "    '''\n",
    "    User can choose to generate data or import data\n",
    "    1: generate data\n",
    "    2: read data\n",
    "    '''\n",
    "    if(datatype == 1):\n",
    "        return generate_data(num_data = 500, step = 1, func = \"Linear\", pattern = 1)\n",
    "    else:\n",
    "        return import_data(dataset = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 3\n",
    "data = read_data(datatype = 1, num_data = 500, step = 1, func = \"Linear\", pattern = 1)\n",
    "\n",
    "train_dataloader, val_dataloader, test_dataloader = build_data_loader(data, seq_length=sequence_length, batch_size=20)\n",
    "\n",
    "# show_data_loader(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data trend visualization\n",
    "##### change them into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-7bb38ba75dfb4bb18f4b08ae7ba588df.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-7bb38ba75dfb4bb18f4b08ae7ba588df.vega-embed details,\n",
       "  #altair-viz-7bb38ba75dfb4bb18f4b08ae7ba588df.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-7bb38ba75dfb4bb18f4b08ae7ba588df\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7bb38ba75dfb4bb18f4b08ae7ba588df\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7bb38ba75dfb4bb18f4b08ae7ba588df\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-afaaf577ca0c76c3ef98981753b60275\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"x\": {\"field\": \"x\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"f(x)\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-afaaf577ca0c76c3ef98981753b60275\": [{\"x\": 0, \"f(x)\": 15}, {\"x\": 1, \"f(x)\": 1}, {\"x\": 2, \"f(x)\": 2}, {\"x\": 3, \"f(x)\": 3}, {\"x\": 4, \"f(x)\": 4}, {\"x\": 5, \"f(x)\": 5}, {\"x\": 6, \"f(x)\": 6}, {\"x\": 7, \"f(x)\": 7}, {\"x\": 8, \"f(x)\": 8}, {\"x\": 9, \"f(x)\": 9}, {\"x\": 10, \"f(x)\": 10}, {\"x\": 11, \"f(x)\": 11}, {\"x\": 12, \"f(x)\": 12}, {\"x\": 13, \"f(x)\": 13}, {\"x\": 14, \"f(x)\": 14}, {\"x\": 15, \"f(x)\": 15}, {\"x\": 16, \"f(x)\": 16}, {\"x\": 17, \"f(x)\": 17}, {\"x\": 18, \"f(x)\": 18}, {\"x\": 19, \"f(x)\": 19}, {\"x\": 20, \"f(x)\": 20}, {\"x\": 21, \"f(x)\": 21}, {\"x\": 22, \"f(x)\": 22}, {\"x\": 23, \"f(x)\": 23}, {\"x\": 24, \"f(x)\": 24}, {\"x\": 25, \"f(x)\": 25}, {\"x\": 26, \"f(x)\": 26}, {\"x\": 27, \"f(x)\": 27}, {\"x\": 28, \"f(x)\": 28}, {\"x\": 29, \"f(x)\": 29}, {\"x\": 30, \"f(x)\": 30}, {\"x\": 31, \"f(x)\": 31}, {\"x\": 32, \"f(x)\": 32}, {\"x\": 33, \"f(x)\": 33}, {\"x\": 34, \"f(x)\": 34}, {\"x\": 35, \"f(x)\": 35}, {\"x\": 36, \"f(x)\": 36}, {\"x\": 37, \"f(x)\": 37}, {\"x\": 38, \"f(x)\": 38}, {\"x\": 39, \"f(x)\": 39}, {\"x\": 40, \"f(x)\": 40}, {\"x\": 41, \"f(x)\": 41}, {\"x\": 42, \"f(x)\": 42}, {\"x\": 43, \"f(x)\": 43}, {\"x\": 44, \"f(x)\": 44}, {\"x\": 45, \"f(x)\": 45}, {\"x\": 46, \"f(x)\": 46}, {\"x\": 47, \"f(x)\": 47}, {\"x\": 48, \"f(x)\": 48}, {\"x\": 49, \"f(x)\": 49}, {\"x\": 50, \"f(x)\": 75}, {\"x\": 51, \"f(x)\": 51}, {\"x\": 52, \"f(x)\": 52}, {\"x\": 53, \"f(x)\": 53}, {\"x\": 54, \"f(x)\": 54}, {\"x\": 55, \"f(x)\": 55}, {\"x\": 56, \"f(x)\": 56}, {\"x\": 57, \"f(x)\": 57}, {\"x\": 58, \"f(x)\": 58}, {\"x\": 59, \"f(x)\": 59}, {\"x\": 60, \"f(x)\": 60}, {\"x\": 61, \"f(x)\": 61}, {\"x\": 62, \"f(x)\": 62}, {\"x\": 63, \"f(x)\": 63}, {\"x\": 64, \"f(x)\": 64}, {\"x\": 65, \"f(x)\": 65}, {\"x\": 66, \"f(x)\": 66}, {\"x\": 67, \"f(x)\": 67}, {\"x\": 68, \"f(x)\": 68}, {\"x\": 69, \"f(x)\": 69}, {\"x\": 70, \"f(x)\": 70}, {\"x\": 71, \"f(x)\": 71}, {\"x\": 72, \"f(x)\": 72}, {\"x\": 73, \"f(x)\": 73}, {\"x\": 74, \"f(x)\": 74}, {\"x\": 75, \"f(x)\": 75}, {\"x\": 76, \"f(x)\": 76}, {\"x\": 77, \"f(x)\": 77}, {\"x\": 78, \"f(x)\": 78}, {\"x\": 79, \"f(x)\": 79}, {\"x\": 80, \"f(x)\": 80}, {\"x\": 81, \"f(x)\": 81}, {\"x\": 82, \"f(x)\": 82}, {\"x\": 83, \"f(x)\": 83}, {\"x\": 84, \"f(x)\": 84}, {\"x\": 85, \"f(x)\": 85}, {\"x\": 86, \"f(x)\": 86}, {\"x\": 87, \"f(x)\": 87}, {\"x\": 88, \"f(x)\": 88}, {\"x\": 89, \"f(x)\": 89}, {\"x\": 90, \"f(x)\": 90}, {\"x\": 91, \"f(x)\": 91}, {\"x\": 92, \"f(x)\": 92}, {\"x\": 93, \"f(x)\": 93}, {\"x\": 94, \"f(x)\": 94}, {\"x\": 95, \"f(x)\": 95}, {\"x\": 96, \"f(x)\": 96}, {\"x\": 97, \"f(x)\": 97}, {\"x\": 98, \"f(x)\": 98}, {\"x\": 99, \"f(x)\": 99}, {\"x\": 100, \"f(x)\": 125}, {\"x\": 101, \"f(x)\": 101}, {\"x\": 102, \"f(x)\": 102}, {\"x\": 103, \"f(x)\": 103}, {\"x\": 104, \"f(x)\": 104}, {\"x\": 105, \"f(x)\": 105}, {\"x\": 106, \"f(x)\": 106}, {\"x\": 107, \"f(x)\": 107}, {\"x\": 108, \"f(x)\": 108}, {\"x\": 109, \"f(x)\": 109}, {\"x\": 110, \"f(x)\": 110}, {\"x\": 111, \"f(x)\": 111}, {\"x\": 112, \"f(x)\": 112}, {\"x\": 113, \"f(x)\": 113}, {\"x\": 114, \"f(x)\": 114}, {\"x\": 115, \"f(x)\": 115}, {\"x\": 116, \"f(x)\": 116}, {\"x\": 117, \"f(x)\": 117}, {\"x\": 118, \"f(x)\": 118}, {\"x\": 119, \"f(x)\": 119}, {\"x\": 120, \"f(x)\": 120}, {\"x\": 121, \"f(x)\": 121}, {\"x\": 122, \"f(x)\": 122}, {\"x\": 123, \"f(x)\": 123}, {\"x\": 124, \"f(x)\": 124}, {\"x\": 125, \"f(x)\": 125}, {\"x\": 126, \"f(x)\": 126}, {\"x\": 127, \"f(x)\": 127}, {\"x\": 128, \"f(x)\": 128}, {\"x\": 129, \"f(x)\": 129}, {\"x\": 130, \"f(x)\": 130}, {\"x\": 131, \"f(x)\": 131}, {\"x\": 132, \"f(x)\": 132}, {\"x\": 133, \"f(x)\": 133}, {\"x\": 134, \"f(x)\": 134}, {\"x\": 135, \"f(x)\": 135}, {\"x\": 136, \"f(x)\": 136}, {\"x\": 137, \"f(x)\": 137}, {\"x\": 138, \"f(x)\": 138}, {\"x\": 139, \"f(x)\": 139}, {\"x\": 140, \"f(x)\": 140}, {\"x\": 141, \"f(x)\": 141}, {\"x\": 142, \"f(x)\": 142}, {\"x\": 143, \"f(x)\": 143}, {\"x\": 144, \"f(x)\": 144}, {\"x\": 145, \"f(x)\": 145}, {\"x\": 146, \"f(x)\": 146}, {\"x\": 147, \"f(x)\": 147}, {\"x\": 148, \"f(x)\": 148}, {\"x\": 149, \"f(x)\": 149}, {\"x\": 150, \"f(x)\": 175}, {\"x\": 151, \"f(x)\": 151}, {\"x\": 152, \"f(x)\": 152}, {\"x\": 153, \"f(x)\": 153}, {\"x\": 154, \"f(x)\": 154}, {\"x\": 155, \"f(x)\": 155}, {\"x\": 156, \"f(x)\": 156}, {\"x\": 157, \"f(x)\": 157}, {\"x\": 158, \"f(x)\": 158}, {\"x\": 159, \"f(x)\": 159}, {\"x\": 160, \"f(x)\": 160}, {\"x\": 161, \"f(x)\": 161}, {\"x\": 162, \"f(x)\": 162}, {\"x\": 163, \"f(x)\": 163}, {\"x\": 164, \"f(x)\": 164}, {\"x\": 165, \"f(x)\": 165}, {\"x\": 166, \"f(x)\": 166}, {\"x\": 167, \"f(x)\": 167}, {\"x\": 168, \"f(x)\": 168}, {\"x\": 169, \"f(x)\": 169}, {\"x\": 170, \"f(x)\": 170}, {\"x\": 171, \"f(x)\": 171}, {\"x\": 172, \"f(x)\": 172}, {\"x\": 173, \"f(x)\": 173}, {\"x\": 174, \"f(x)\": 174}, {\"x\": 175, \"f(x)\": 175}, {\"x\": 176, \"f(x)\": 176}, {\"x\": 177, \"f(x)\": 177}, {\"x\": 178, \"f(x)\": 178}, {\"x\": 179, \"f(x)\": 179}, {\"x\": 180, \"f(x)\": 180}, {\"x\": 181, \"f(x)\": 181}, {\"x\": 182, \"f(x)\": 182}, {\"x\": 183, \"f(x)\": 183}, {\"x\": 184, \"f(x)\": 184}, {\"x\": 185, \"f(x)\": 185}, {\"x\": 186, \"f(x)\": 186}, {\"x\": 187, \"f(x)\": 187}, {\"x\": 188, \"f(x)\": 188}, {\"x\": 189, \"f(x)\": 189}, {\"x\": 190, \"f(x)\": 190}, {\"x\": 191, \"f(x)\": 191}, {\"x\": 192, \"f(x)\": 192}, {\"x\": 193, \"f(x)\": 193}, {\"x\": 194, \"f(x)\": 194}, {\"x\": 195, \"f(x)\": 195}, {\"x\": 196, \"f(x)\": 196}, {\"x\": 197, \"f(x)\": 197}, {\"x\": 198, \"f(x)\": 198}, {\"x\": 199, \"f(x)\": 199}, {\"x\": 200, \"f(x)\": 225}, {\"x\": 201, \"f(x)\": 201}, {\"x\": 202, \"f(x)\": 202}, {\"x\": 203, \"f(x)\": 203}, {\"x\": 204, \"f(x)\": 204}, {\"x\": 205, \"f(x)\": 205}, {\"x\": 206, \"f(x)\": 206}, {\"x\": 207, \"f(x)\": 207}, {\"x\": 208, \"f(x)\": 208}, {\"x\": 209, \"f(x)\": 209}, {\"x\": 210, \"f(x)\": 210}, {\"x\": 211, \"f(x)\": 211}, {\"x\": 212, \"f(x)\": 212}, {\"x\": 213, \"f(x)\": 213}, {\"x\": 214, \"f(x)\": 214}, {\"x\": 215, \"f(x)\": 215}, {\"x\": 216, \"f(x)\": 216}, {\"x\": 217, \"f(x)\": 217}, {\"x\": 218, \"f(x)\": 218}, {\"x\": 219, \"f(x)\": 219}, {\"x\": 220, \"f(x)\": 220}, {\"x\": 221, \"f(x)\": 221}, {\"x\": 222, \"f(x)\": 222}, {\"x\": 223, \"f(x)\": 223}, {\"x\": 224, \"f(x)\": 224}, {\"x\": 225, \"f(x)\": 225}, {\"x\": 226, \"f(x)\": 226}, {\"x\": 227, \"f(x)\": 227}, {\"x\": 228, \"f(x)\": 228}, {\"x\": 229, \"f(x)\": 229}, {\"x\": 230, \"f(x)\": 230}, {\"x\": 231, \"f(x)\": 231}, {\"x\": 232, \"f(x)\": 232}, {\"x\": 233, \"f(x)\": 233}, {\"x\": 234, \"f(x)\": 234}, {\"x\": 235, \"f(x)\": 235}, {\"x\": 236, \"f(x)\": 236}, {\"x\": 237, \"f(x)\": 237}, {\"x\": 238, \"f(x)\": 238}, {\"x\": 239, \"f(x)\": 239}, {\"x\": 240, \"f(x)\": 240}, {\"x\": 241, \"f(x)\": 241}, {\"x\": 242, \"f(x)\": 242}, {\"x\": 243, \"f(x)\": 243}, {\"x\": 244, \"f(x)\": 244}, {\"x\": 245, \"f(x)\": 245}, {\"x\": 246, \"f(x)\": 246}, {\"x\": 247, \"f(x)\": 247}, {\"x\": 248, \"f(x)\": 248}, {\"x\": 249, \"f(x)\": 249}, {\"x\": 250, \"f(x)\": 275}, {\"x\": 251, \"f(x)\": 251}, {\"x\": 252, \"f(x)\": 252}, {\"x\": 253, \"f(x)\": 253}, {\"x\": 254, \"f(x)\": 254}, {\"x\": 255, \"f(x)\": 255}, {\"x\": 256, \"f(x)\": 256}, {\"x\": 257, \"f(x)\": 257}, {\"x\": 258, \"f(x)\": 258}, {\"x\": 259, \"f(x)\": 259}, {\"x\": 260, \"f(x)\": 260}, {\"x\": 261, \"f(x)\": 261}, {\"x\": 262, \"f(x)\": 262}, {\"x\": 263, \"f(x)\": 263}, {\"x\": 264, \"f(x)\": 264}, {\"x\": 265, \"f(x)\": 265}, {\"x\": 266, \"f(x)\": 266}, {\"x\": 267, \"f(x)\": 267}, {\"x\": 268, \"f(x)\": 268}, {\"x\": 269, \"f(x)\": 269}, {\"x\": 270, \"f(x)\": 270}, {\"x\": 271, \"f(x)\": 271}, {\"x\": 272, \"f(x)\": 272}, {\"x\": 273, \"f(x)\": 273}, {\"x\": 274, \"f(x)\": 274}, {\"x\": 275, \"f(x)\": 275}, {\"x\": 276, \"f(x)\": 276}, {\"x\": 277, \"f(x)\": 277}, {\"x\": 278, \"f(x)\": 278}, {\"x\": 279, \"f(x)\": 279}, {\"x\": 280, \"f(x)\": 280}, {\"x\": 281, \"f(x)\": 281}, {\"x\": 282, \"f(x)\": 282}, {\"x\": 283, \"f(x)\": 283}, {\"x\": 284, \"f(x)\": 284}, {\"x\": 285, \"f(x)\": 285}, {\"x\": 286, \"f(x)\": 286}, {\"x\": 287, \"f(x)\": 287}, {\"x\": 288, \"f(x)\": 288}, {\"x\": 289, \"f(x)\": 289}, {\"x\": 290, \"f(x)\": 290}, {\"x\": 291, \"f(x)\": 291}, {\"x\": 292, \"f(x)\": 292}, {\"x\": 293, \"f(x)\": 293}, {\"x\": 294, \"f(x)\": 294}, {\"x\": 295, \"f(x)\": 295}, {\"x\": 296, \"f(x)\": 296}, {\"x\": 297, \"f(x)\": 297}, {\"x\": 298, \"f(x)\": 298}, {\"x\": 299, \"f(x)\": 299}, {\"x\": 300, \"f(x)\": 325}, {\"x\": 301, \"f(x)\": 301}, {\"x\": 302, \"f(x)\": 302}, {\"x\": 303, \"f(x)\": 303}, {\"x\": 304, \"f(x)\": 304}, {\"x\": 305, \"f(x)\": 305}, {\"x\": 306, \"f(x)\": 306}, {\"x\": 307, \"f(x)\": 307}, {\"x\": 308, \"f(x)\": 308}, {\"x\": 309, \"f(x)\": 309}, {\"x\": 310, \"f(x)\": 310}, {\"x\": 311, \"f(x)\": 311}, {\"x\": 312, \"f(x)\": 312}, {\"x\": 313, \"f(x)\": 313}, {\"x\": 314, \"f(x)\": 314}, {\"x\": 315, \"f(x)\": 315}, {\"x\": 316, \"f(x)\": 316}, {\"x\": 317, \"f(x)\": 317}, {\"x\": 318, \"f(x)\": 318}, {\"x\": 319, \"f(x)\": 319}, {\"x\": 320, \"f(x)\": 320}, {\"x\": 321, \"f(x)\": 321}, {\"x\": 322, \"f(x)\": 322}, {\"x\": 323, \"f(x)\": 323}, {\"x\": 324, \"f(x)\": 324}, {\"x\": 325, \"f(x)\": 325}, {\"x\": 326, \"f(x)\": 326}, {\"x\": 327, \"f(x)\": 327}, {\"x\": 328, \"f(x)\": 328}, {\"x\": 329, \"f(x)\": 329}, {\"x\": 330, \"f(x)\": 330}, {\"x\": 331, \"f(x)\": 331}, {\"x\": 332, \"f(x)\": 332}, {\"x\": 333, \"f(x)\": 333}, {\"x\": 334, \"f(x)\": 334}, {\"x\": 335, \"f(x)\": 335}, {\"x\": 336, \"f(x)\": 336}, {\"x\": 337, \"f(x)\": 337}, {\"x\": 338, \"f(x)\": 338}, {\"x\": 339, \"f(x)\": 339}, {\"x\": 340, \"f(x)\": 340}, {\"x\": 341, \"f(x)\": 341}, {\"x\": 342, \"f(x)\": 342}, {\"x\": 343, \"f(x)\": 343}, {\"x\": 344, \"f(x)\": 344}, {\"x\": 345, \"f(x)\": 345}, {\"x\": 346, \"f(x)\": 346}, {\"x\": 347, \"f(x)\": 347}, {\"x\": 348, \"f(x)\": 348}, {\"x\": 349, \"f(x)\": 349}, {\"x\": 350, \"f(x)\": 375}, {\"x\": 351, \"f(x)\": 351}, {\"x\": 352, \"f(x)\": 352}, {\"x\": 353, \"f(x)\": 353}, {\"x\": 354, \"f(x)\": 354}, {\"x\": 355, \"f(x)\": 355}, {\"x\": 356, \"f(x)\": 356}, {\"x\": 357, \"f(x)\": 357}, {\"x\": 358, \"f(x)\": 358}, {\"x\": 359, \"f(x)\": 359}, {\"x\": 360, \"f(x)\": 360}, {\"x\": 361, \"f(x)\": 361}, {\"x\": 362, \"f(x)\": 362}, {\"x\": 363, \"f(x)\": 363}, {\"x\": 364, \"f(x)\": 364}, {\"x\": 365, \"f(x)\": 365}, {\"x\": 366, \"f(x)\": 366}, {\"x\": 367, \"f(x)\": 367}, {\"x\": 368, \"f(x)\": 368}, {\"x\": 369, \"f(x)\": 369}, {\"x\": 370, \"f(x)\": 370}, {\"x\": 371, \"f(x)\": 371}, {\"x\": 372, \"f(x)\": 372}, {\"x\": 373, \"f(x)\": 373}, {\"x\": 374, \"f(x)\": 374}, {\"x\": 375, \"f(x)\": 375}, {\"x\": 376, \"f(x)\": 376}, {\"x\": 377, \"f(x)\": 377}, {\"x\": 378, \"f(x)\": 378}, {\"x\": 379, \"f(x)\": 379}, {\"x\": 380, \"f(x)\": 380}, {\"x\": 381, \"f(x)\": 381}, {\"x\": 382, \"f(x)\": 382}, {\"x\": 383, \"f(x)\": 383}, {\"x\": 384, \"f(x)\": 384}, {\"x\": 385, \"f(x)\": 385}, {\"x\": 386, \"f(x)\": 386}, {\"x\": 387, \"f(x)\": 387}, {\"x\": 388, \"f(x)\": 388}, {\"x\": 389, \"f(x)\": 389}, {\"x\": 390, \"f(x)\": 390}, {\"x\": 391, \"f(x)\": 391}, {\"x\": 392, \"f(x)\": 392}, {\"x\": 393, \"f(x)\": 393}, {\"x\": 394, \"f(x)\": 394}, {\"x\": 395, \"f(x)\": 395}, {\"x\": 396, \"f(x)\": 396}, {\"x\": 397, \"f(x)\": 397}, {\"x\": 398, \"f(x)\": 398}, {\"x\": 399, \"f(x)\": 399}, {\"x\": 400, \"f(x)\": 425}, {\"x\": 401, \"f(x)\": 401}, {\"x\": 402, \"f(x)\": 402}, {\"x\": 403, \"f(x)\": 403}, {\"x\": 404, \"f(x)\": 404}, {\"x\": 405, \"f(x)\": 405}, {\"x\": 406, \"f(x)\": 406}, {\"x\": 407, \"f(x)\": 407}, {\"x\": 408, \"f(x)\": 408}, {\"x\": 409, \"f(x)\": 409}, {\"x\": 410, \"f(x)\": 410}, {\"x\": 411, \"f(x)\": 411}, {\"x\": 412, \"f(x)\": 412}, {\"x\": 413, \"f(x)\": 413}, {\"x\": 414, \"f(x)\": 414}, {\"x\": 415, \"f(x)\": 415}, {\"x\": 416, \"f(x)\": 416}, {\"x\": 417, \"f(x)\": 417}, {\"x\": 418, \"f(x)\": 418}, {\"x\": 419, \"f(x)\": 419}, {\"x\": 420, \"f(x)\": 420}, {\"x\": 421, \"f(x)\": 421}, {\"x\": 422, \"f(x)\": 422}, {\"x\": 423, \"f(x)\": 423}, {\"x\": 424, \"f(x)\": 424}, {\"x\": 425, \"f(x)\": 425}, {\"x\": 426, \"f(x)\": 426}, {\"x\": 427, \"f(x)\": 427}, {\"x\": 428, \"f(x)\": 428}, {\"x\": 429, \"f(x)\": 429}, {\"x\": 430, \"f(x)\": 430}, {\"x\": 431, \"f(x)\": 431}, {\"x\": 432, \"f(x)\": 432}, {\"x\": 433, \"f(x)\": 433}, {\"x\": 434, \"f(x)\": 434}, {\"x\": 435, \"f(x)\": 435}, {\"x\": 436, \"f(x)\": 436}, {\"x\": 437, \"f(x)\": 437}, {\"x\": 438, \"f(x)\": 438}, {\"x\": 439, \"f(x)\": 439}, {\"x\": 440, \"f(x)\": 440}, {\"x\": 441, \"f(x)\": 441}, {\"x\": 442, \"f(x)\": 442}, {\"x\": 443, \"f(x)\": 443}, {\"x\": 444, \"f(x)\": 444}, {\"x\": 445, \"f(x)\": 445}, {\"x\": 446, \"f(x)\": 446}, {\"x\": 447, \"f(x)\": 447}, {\"x\": 448, \"f(x)\": 448}, {\"x\": 449, \"f(x)\": 449}, {\"x\": 450, \"f(x)\": 475}, {\"x\": 451, \"f(x)\": 451}, {\"x\": 452, \"f(x)\": 452}, {\"x\": 453, \"f(x)\": 453}, {\"x\": 454, \"f(x)\": 454}, {\"x\": 455, \"f(x)\": 455}, {\"x\": 456, \"f(x)\": 456}, {\"x\": 457, \"f(x)\": 457}, {\"x\": 458, \"f(x)\": 458}, {\"x\": 459, \"f(x)\": 459}, {\"x\": 460, \"f(x)\": 460}, {\"x\": 461, \"f(x)\": 461}, {\"x\": 462, \"f(x)\": 462}, {\"x\": 463, \"f(x)\": 463}, {\"x\": 464, \"f(x)\": 464}, {\"x\": 465, \"f(x)\": 465}, {\"x\": 466, \"f(x)\": 466}, {\"x\": 467, \"f(x)\": 467}, {\"x\": 468, \"f(x)\": 468}, {\"x\": 469, \"f(x)\": 469}, {\"x\": 470, \"f(x)\": 470}, {\"x\": 471, \"f(x)\": 471}, {\"x\": 472, \"f(x)\": 472}, {\"x\": 473, \"f(x)\": 473}, {\"x\": 474, \"f(x)\": 474}, {\"x\": 475, \"f(x)\": 475}, {\"x\": 476, \"f(x)\": 476}, {\"x\": 477, \"f(x)\": 477}, {\"x\": 478, \"f(x)\": 478}, {\"x\": 479, \"f(x)\": 479}, {\"x\": 480, \"f(x)\": 480}, {\"x\": 481, \"f(x)\": 481}, {\"x\": 482, \"f(x)\": 482}, {\"x\": 483, \"f(x)\": 483}, {\"x\": 484, \"f(x)\": 484}, {\"x\": 485, \"f(x)\": 485}, {\"x\": 486, \"f(x)\": 486}, {\"x\": 487, \"f(x)\": 487}, {\"x\": 488, \"f(x)\": 488}, {\"x\": 489, \"f(x)\": 489}, {\"x\": 490, \"f(x)\": 490}, {\"x\": 491, \"f(x)\": 491}, {\"x\": 492, \"f(x)\": 492}, {\"x\": 493, \"f(x)\": 493}, {\"x\": 494, \"f(x)\": 494}, {\"x\": 495, \"f(x)\": 495}, {\"x\": 496, \"f(x)\": 496}, {\"x\": 497, \"f(x)\": 497}, {\"x\": 498, \"f(x)\": 498}, {\"x\": 499, \"f(x)\": 499}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Change \n",
    "import altair as alt\n",
    "\n",
    "source = pd.DataFrame({\n",
    "    'x': np.arange(0,len(data)),\n",
    "    'f(x)': data\n",
    "  })\n",
    "\n",
    "alt.Chart(source).mark_line().encode(\n",
    "      x='x',\n",
    "      y='f(x)'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bundle(train_dataloader, val_dataloader, test_dataloader, seq_len = sequence_length, model_type = 4, num_epoch = 1000, verbose = True):\n",
    "    '''\n",
    "    Model bundle function that takes \"Linear\", \"MLP\", \"CNN\", \"LSTM\" as input\n",
    "    \"Linear\": 1\n",
    "    \"MLP\": 2\n",
    "    \"CNN\": 3\n",
    "    \"LSTM\": 4\n",
    "    If \"verbose\", output the model summary\n",
    "    '''\n",
    "    if(model_type == 1):\n",
    "        model = linear_Net(seq_len, 1)\n",
    "    elif(model_type == 2):\n",
    "        model = mlp_Net(seq_len, hidden_size = 32, output_size = 1)\n",
    "    elif(model_type == 3):\n",
    "        model = cnn_Net(seq_len, kernel_size = 3, output_size = 1)\n",
    "    elif(model_type == 4):\n",
    "        model = lstm_Net(seq_len, hidden_size = 32, output_size = 1)\n",
    "\n",
    "    if(verbose and model_type < 4):\n",
    "        print(\"+--------------------+\")\n",
    "        print(\"|Print the linear model summary|\")\n",
    "        print(\"+--------------------+\")\n",
    "        print(summary(model, (1, seq_len)))\n",
    "    \n",
    "    ## train the model\n",
    "    train_losses, val_losses, test_losses = model_run(train_dataloader, val_dataloader, test_dataloader, model, device, num_epochs = num_epoch)\n",
    "\n",
    "    return train_losses, val_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 | train loss: 20019.207 | val loss: 89711.287\n",
      "Epoch1 | train loss: 19951.585 | val loss: 89618.014\n",
      "Epoch2 | train loss: 19882.360 | val loss: 89436.156\n",
      "Epoch3 | train loss: 19777.610 | val loss: 89072.037\n",
      "Epoch4 | train loss: 19616.271 | val loss: 88754.042\n",
      "Epoch5 | train loss: 19549.158 | val loss: 88611.432\n",
      "Epoch6 | train loss: 19460.380 | val loss: 88384.397\n",
      "Epoch7 | train loss: 19399.312 | val loss: 88243.109\n",
      "Epoch8 | train loss: 19298.390 | val loss: 87932.287\n",
      "Epoch9 | train loss: 19206.060 | val loss: 87650.205\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, test_losses = model_bundle(train_dataloader = train_dataloader, val_dataloader=val_dataloader, test_dataloader = test_dataloader, seq_len=sequence_length, model_type=4, num_epoch=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_loss_plot(train_losses, val_losses, model_type = 4, loss_type = \"MSE Loss\", sparse_n = 0):\n",
    "    '''\n",
    "    For each train/test loss trajectory, plot loss by epoch\n",
    "    '''\n",
    "    model_dict = {\n",
    "        1: \"Linear\",\n",
    "        2: \"MLP\",\n",
    "        3: \"CNN\",\n",
    "        4: \"LSTM\"\n",
    "    }\n",
    "\n",
    "    data_label_list = [(train_losses, val_losses, model_dict[model_type])]\n",
    "\n",
    "    for i,(train_data,val_data,label) in enumerate(data_label_list):    \n",
    "        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n",
    "        plt.plot(val_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel(loss_type)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_prediction_plot(data_label_list, sparse_n = 0):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test with testing data\n",
    "with torch.no_grad():\n",
    "    # shift train predictions for plotting\n",
    "    train_plot = np.ones_like(data) * np.nan\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = y_pred[:, -1, :]\n",
    "    train_plot[lookback:train_size] = model(X_train)[:, -1, :]\n",
    "    # shift test predictions for plotting\n",
    "    test_plot = np.ones_like(data) * np.nan\n",
    "    test_plot[train_size+lookback:len(data)] = model(X_test)[:, -1, :]\n",
    "# plot\n",
    "plt.plot(data, c='b')\n",
    "plt.plot(train_plot, c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
